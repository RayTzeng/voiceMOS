runner:
  total_steps: 20000
  gradient_clipping: 5
  gradient_accumulate_steps: 8

  log_step: 100
  eval_step: 500
  save_step: 500
  max_keep: 1
  eval_dataloaders:
    - eval
    - test

optimizer:
  name: Adam
  lr: 1.0e-5

# comment the whole scheduler config block
# to disable learning rate scheduling
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 500

downstream_expert:
  datarc:
    num_workers: 1
    train_batch_size: 2
    eval_batch_size: 2
    train_dataset: ['BVCC'] # BVCC, NISQA
    eval_dataset: ['BVCC']
    test_dataset: ['BVCC','NISQA']
    BVCC:
      base_path: /home/raytz/Disk/BVCC/DATA
      train_csv: sets/train_mos_list.txt
      eval_csv: sets/val_mos_list.txt
      test_csv: sets/test_mos_list.txt

    NISQA:
      base_path: /home/raytz/Disk/NISQA_Corpus
      csv_file: NISQA_corpus_file.csv
      train_split: ["NISQA_TRAIN_LIVE", "NISQA_TRAIN_SIM"]
      eval_split: ["NISQA_VAL_LIVE", "NISQA_VAL_SIM"]
      test_split: ["NISQA_TEST_LIVETALK", "NISQA_TEST_P501", "NISQA_TEST_FOR"]

  modelrc:
    projector_dim: 768
    objective: MSELoss
    dim: [768, 256]
    dropout: [0, 0]
    activation: ReLU
    pooling_name: AttentivePooling
